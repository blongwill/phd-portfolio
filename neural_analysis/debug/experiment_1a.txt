DATASETS: ['src/../datasets/test_english_dataset.txt']###
Experiment config settings:
config_name: experiment_1a
random_seed: 121
dataset: test_english_dataset
model_type: bert-base-uncased
optimizer: AdamW
learning_rate: 2e-5
epsilon: 1e-8
num_training_epochs: 2
num_warmup_steps: 0
num_classifier_labels: 4
output_attentions: False
output_hidden_states: False
batch_size: 16
max_tokens_length: 43
###
Starting experiment execution
There are 1 GPU(s) available.
We will use the GPU:Tesla M10
Starting load dataset
First data label: 1
Finished load dataset
Loading BERT tokenizer...
Starting split dataset
Input IDs dimension-1 size: 108
Input IDs dimension-2 size: 43
Input Labels dimension-1 size: 108
Input Masks dimension-1 size: 108
Input Masks dimension-2 size: 43
Train inputs dimension-1 size: 97
Train inputs dimension-2 size: 43
Train labels dimension-1 size: 97
Train masks dimension-1 size: 97
Train masks dimension-2 size: 43
Finished split dataset
Made it here
Starting to Load BertForSequenceClassification
Finished initializing model
Starting train hate speech model

======== Epoch 1 / 2 ========
Training...

  Average training loss: 1.21
  Training epcoh took: 0:00:04

======== Epoch 2 / 2 ========
Training...

  Average training loss: 0.99
  Training epcoh took: 0:00:04

Training complete!
Finished train hate speech model
Starting evaluation: test data
  Accuracy: 0.64
  F1: 0.49
  Validation took: 0:00:00
Finished evaluation: test data
Starting visualization of trained model
Loading BERT tokenizer...
There are 1 GPU(s) available.
We will use the GPU:Tesla M10
Finished visualization of trained model given input The cat sat on the dog
HTML file saved to viz/experiment_1a/The cat sat on the dog
Finished experiment execution
