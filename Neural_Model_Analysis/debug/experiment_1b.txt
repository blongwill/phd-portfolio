DATASETS: ['src/../datasets/toy_dataset.txt']###
Experiment config settings:
config_name: experiment_1b
random_seed: 121
dataset: toy_dataset
model_type: bert-base-uncased
optimizer: AdamW
learning_rate: 2e-5
epsilon: 1e-8
num_training_epochs: 2
num_warmup_steps: 0
num_classifier_labels: 4
output_attentions: True
output_hidden_states: True
batch_size: 16
max_tokens_length: 40
###
Starting experiment execution
No GPU available, using the CPU instead.
Starting load dataset
First data label: 1
Finished load dataset
Loading BERT tokenizer...
Starting split dataset
Input IDs dimension-1 size: 10
Input IDs dimension-2 size: 40
Input Labels dimension-1 size: 10
Input Masks dimension-1 size: 10
Input Masks dimension-2 size: 40
Train inputs dimension-1 size: 9
Train inputs dimension-2 size: 40
Train labels dimension-1 size: 9
Train masks dimension-1 size: 9
Train masks dimension-2 size: 40
Finished split dataset
Made it here
Starting to Load BertForSequenceClassification
Finished initializing model
Starting train hate speech model

======== Epoch 1 / 2 ========
Training...
