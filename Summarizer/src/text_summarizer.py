#!/usr/bin/python
# -*- coding: utf-8 -*-

"""Nutshell, a topic-focused multi-document extractive summarization system."""

__author__ = 'Shannon Ladymon, Haley Lepp, Ben Longwill, Amina Venton'
__email__ = \
    'sladymon@uw.edu, hlepp@uw.edu, longwill@uw.edu, aventon@uw.edu'

from data_input import get_data, get_gold_standard_docs
from content_selection import select_content
from info_ordering import order_info_chron, order_info_entity, get_training_vectors
from evaluation import eval_summary
from sys import argv
import argparse
import os


def write_summary_files(topics_with_final_summaries, output_folder):
    """
    This function takes in a list of topics with summaries
    generated by realize_content(topics_with_summaries_in_order)
    in text_summarizer.py main.
    It outputs a file for each topic with the final summary.
    """

    # Directory where output files should be

    output_dir = 'outputs/' + output_folder + '/'

    # Variable to create unique ending for files
    numeric_count = 1

    for topic in topics_with_final_summaries:

        # Get topic id for each Topic object
        topic_id = topic.topic_id 
       
        # Split topic ID into 2 parts
        id_part1 = topic_id[:-1]
        id_part2 = topic_id[-1:]

        # Make output file name and directory
        file_path = os.path.join(output_dir
                                 + '{}-A.M.100.{}.{}'.format(id_part1,
                                 id_part2, str(numeric_count)))
        directory = os.path.dirname(file_path)

        if not os.path.exists(directory):
            os.makedirs(directory)
        
        # Write sentences to topic output file
        with open(file_path, 'w') as out_file:

            # Get list of summary Sentence objects from the Topic object
            summary_sentences = topic.summary
            
            for sentence in summary_sentences:
                out_file.write(sentence.original_sentence + '\n')


def summarize_topics_list(topics, output_folder, test_type, d, intersent_threshold, summary_threshold, epsilon, mle_lambda, k, min_sent_len, include_narrative, bias_formula, intersent_formula, info_order_type, num_permutations):
    """
    Creates extractive summaries (<= 100 words) of multi-document news sets for a list of Topics
    Prints one summary file per topic and nests inside outputs/<output_folder>/
    Runs ROUGE and prints results file inside results/<output_folder>_rouge_scores.out

    Args:
        topics: a list of Topic objects (which include Documents and Sentences)
        output_folder: name of folder to write summaries to
        test_type: either 'dev' for devtest data, or 'eval' for evaltest data
        d: damping factor, amount to prioritize topic bias in Markov Matrix
        intersent_threshold: minimum amount of similarity required to include in Similarity Matrix
        summary_threshold: maximum amount of similarity between sentences in summary
        epsilon: minimum amount of difference between probabilities between rounds of power method
        mle_lambda: amount to prioritize topic MLE over sentence MLE 
        k: maximum number of intersentential similarity nodes to connect when doing normalized generation probability
        min_sent_len: minimum number of words in a sentence to be used in the summary
        include_narrative: True if the narrative (in addition to title) should be in the bias
        bias_formula: which formula to use for sentence-topic similarity weighting - cos (cosine similarity), rel (relevance), or gen (generation probability)
        intersent_formula: which formula to use for inter-sentential similarity weighting - cos (cosine similarity) or norm (normalized generation probability)
        info_order_type: entity or chron
        num_permutations: int for how many SVM permutations

    Returns:
        topic_list: the modified topic_list from the input, with a list of selected sentences
        in the topic.summary fields of each topic.

    """
    
    # Content Selection
    # identifies salient sentences & ranks them
    # & chooses up to 100 words (using full sentences)
    # Returns the list of topics with each topic.summary variable
    # modified to include a list of sentences to include

    topics_with_summaries = select_content(topics, d, intersent_threshold, summary_threshold, epsilon, mle_lambda, k, min_sent_len, include_narrative, bias_formula, intersent_formula)

    # Information Ordering
    
    if info_order_type == "entity":
        
        # Entity-Based Approach
        # Orders sentences by best ranked from entity grid model for each topic
        # Returns the list of topics with optimally ordered
        # sentences in each topic.summary variable

        topics_with_summaries_in_order = order_info_entity(topics_with_summaries, num_permutations, output_folder)
    else:

        # Chronological Order Approach
        # Orders sentences by date and sentence position for each topic
        # Returns the list of topics with chronologically ordered
        # sentences in each topic.summary variable

        topics_with_summaries_in_order = order_info_chron(topics_with_summaries)


    # Writes summaries to file for each topic
    write_summary_files(topics_with_summaries_in_order, output_folder)

    # Evaluates summaries for each topic
    # by running ROUGE-1 & ROUGE-2

    eval_summary(output_folder, test_type)



def summarize_text(file_path, output_folder, test_type, stemming, lower, idf_type, tf_type, d, intersent_threshold, summary_threshold, epsilon, mle_lambda, k, min_sent_len, include_narrative, bias_formula, intersent_formula, info_order_type, num_permutations, remove_header, remove_parens, remove_quotes, remove_appos, remove_advcl, remove_relcl, remove_acl):
    """
    Creates extractive summaries (<= 100 words) of multi-document news sets from TAC 2009/2010
    Prints one summary file per topic and nests inside outputs/<output_folder>/
    Runs ROUGE and prints results file inside results/<output_folder>_rouge_scores.out

    Args:
        file_path:str file path on patas that leads to directory that holds training or testing data
        output_folder: name of folder to write summaries to
        test_type: either 'dev' for devtest data, or 'eval' for evaltest data
        stemming:bool True enables each sentence to be stored with a stem representation in objects and tokens, False does nothing
        lower:bool True enables each sentence to be stored in lower case, False does nothing.
        idf_type:str String input dictates idf representation in objects. Options are: 'smooth_idf', 'probabilistic_idf' , 'standard_idf' , and 'unary_idf'
        tf_type:str String input dictates tf representation in objects. Options are: 'term_frequency', 'log_normalization'
        d: damping factor, amount to prioritize topic bias in Markov Matrix
        intersent_threshold: minimum amount of similarity required to include in Similarity Matrix
        summary_threshold: maximum amount of similarity between sentences in summary
        epsilon: minimum amount of difference between probabilities between rounds of power method
        mle_lambda: amount to prioritize topic MLE over sentence MLE 
        k: maximum number of intersentential similarity nodes to connect when doing normalized generation probability
        min_sent_len: minimum number of words in a sentence to be used in the summary
        include_narrative: True if the narrative (in addition to title) should be in the bias
        bias_formula: which formula to use for sentence-topic similarity weighting - cos (cosine similarity), rel (relevance), or gen (generation probability)
        intersent_formula: which formula to use for inter-sentential similarity weighting - cos (cosine similarity) or norm (normalized generation probability)
        info_order_type: entity or chron
        num_permutations: int for how many SVM permutations
        remove_header:bool True if the header should be removed in sentence compression
        remove_parens:bool True if parenthetical information should be removed in sentence compression
        remove_quotes:bool True if unpaired quotes should be removed in sentence compression
        remove_appos:bool True if appositional modifier should be removed in sentence compression
        remove_advcl:bool True if adverbial clause modifier should be removed in sentence compression
        remove_relcl:bool True if relative clause modifier should be removed in sentence compression
        remove_acl: True if a finite or non-finite clausal modifier should be removed in in sentence compression

    Returns:
        topic_list: the modified topic_list from the input, with a list of selected sentences
        in the topic.summary fields of each topic.

    """
    
    # Read in input data
    # and handle content realization as a pre-processing step
    # and return a list of Topic objects (with Documents/Sentences)
    topics = get_data(file_path, stemming, lower, idf_type, tf_type, remove_header, remove_parens, remove_quotes, remove_appos, remove_advcl, remove_relcl, remove_acl)
#    topics = get_data(file_path, stemming, lower, idf_type, tf_type)

    summarize_topics_list(topics, output_folder, test_type, d, intersent_threshold, summary_threshold, epsilon, mle_lambda, k, min_sent_len, include_narrative, bias_formula, intersent_formula, info_order_type, num_permutations)


if __name__ == '__main__':

    # Grab arguments
    p = argparse.ArgumentParser()
    p.add_argument('dev_file')
    p.add_argument('eval_file')
    p.add_argument('output_folder')
    p.add_argument('test_type')
    p.add_argument('stemming')
    p.add_argument('lower')
    p.add_argument('idf_type')
    p.add_argument('tf_type')
    p.add_argument('d')
    p.add_argument('intersent_threshold')
    p.add_argument('summary_threshold')
    p.add_argument('epsilon')
    p.add_argument('mle_lambda')
    p.add_argument('k')
    p.add_argument('min_sent_len')
    p.add_argument('include_narrative')
    p.add_argument('bias_formula')
    p.add_argument('intersent_formula')
    p.add_argument('info_order_type')
    p.add_argument('num_permutations')
    p.add_argument('remove_header')
    p.add_argument('remove_parens')
    p.add_argument('remove_quotes')
    p.add_argument('remove_appos')
    p.add_argument('remove_advcl')
    p.add_argument('remove_relcl')
    p.add_argument('remove_acl')
    args = p.parse_args()
 
    dev_path = str(args.dev_file)
    eval_path = str(args.eval_file)
    output_folder = str(args.output_folder)
    test_type = str(args.test_type)
    stemming = bool(int(args.stemming))
    lower = bool(int(args.lower))
    idf_type = str(args.idf_type)
    tf_type = str(args.tf_type)
    d = float(args.d)
    intersent_threshold = float(args.intersent_threshold)
    summary_threshold = float(args.summary_threshold)
    epsilon = float(args.epsilon)
    mle_lambda = float(args.mle_lambda)
    k = int(args.k)
    min_sent_len = int(args.min_sent_len)
    include_narrative = bool(int(args.include_narrative))
    bias_formula = str(args.bias_formula)
    intersent_formula = str(args.intersent_formula)
    info_order_type = str(args.info_order_type)
    num_permutations = int(args.num_permutations)
    remove_header = bool(int(args.remove_header))
    remove_parens = bool(int(args.remove_parens))
    remove_quotes = bool(int(args.remove_quotes))
    remove_appos = bool(int(args.remove_appos))
    remove_advcl = bool(int(args.remove_advcl))
    remove_relcl = bool(int(args.remove_relcl))
    remove_acl = bool(int(args.remove_acl))

    dev_output_folder = output_folder + "_devtest"
    eval_output_folder = output_folder + "_evaltest"

    # Run summarizer on either dev, eval, or both depending on test_type
    if test_type == "dev":

        # Run the text summarizer on dev data with the given parameters
        summarize_text(dev_path, dev_output_folder, test_type, stemming, lower, idf_type, tf_type, d, intersent_threshold, summary_threshold, epsilon, mle_lambda, k, min_sent_len, include_narrative, bias_formula, intersent_formula, info_order_type, num_permutations, remove_header, remove_parens, remove_quotes, remove_appos, remove_advcl, remove_relcl, remove_acl)	

    elif test_type == "eval":

        # Run the text summarizer on eval data with the given parameters
        summarize_text(eval_path, eval_output_folder, test_type, stemming, lower, idf_type, tf_type, d, intersent_threshold, summary_threshold, epsilon, mle_lambda, k, min_sent_len, include_narrative, bias_formula, intersent_formula, info_order_type, num_permutations, remove_header, remove_parens, remove_quotes, remove_appos, remove_advcl, remove_relcl, remove_acl)	

    else:

        # Run the text summarizer on dev data with the given parameters
        summarize_text(dev_path, dev_output_folder, "dev", stemming, lower, idf_type, tf_type, d, intersent_threshold, summary_threshold, epsilon, mle_lambda, k, min_sent_len, include_narrative, bias_formula, intersent_formula, info_order_type, num_permutations, remove_header, remove_parens, remove_quotes, remove_appos, remove_advcl, remove_relcl, remove_acl)	

        # Run the text summarizer on eval data with the given parameters
        summarize_text(eval_path, eval_output_folder, "eval", stemming, lower, idf_type, tf_type, d, intersent_threshold, summary_threshold, epsilon, mle_lambda, k, min_sent_len, include_narrative, bias_formula, intersent_formula, info_order_type, num_permutations, remove_header, remove_parens, remove_quotes, remove_appos, remove_advcl, remove_relcl, remove_acl)	
